% This work is licensed under the Creative Commons
% Attribution-NonCommercial-ShareAlike 4.0 International License. To view a copy
% of this license, visit http://creativecommons.org/licenses/by-nc-sa/4.0/ or
% send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.

\documentclass[12pt,a4paper]{article} 

\input{../../latex/packages}
\input{../../latex/theoremenvironments}
\input{../../latex/commands}
\input{../../latex/commands_Willi}
\input{../../latex/commands_stochastik}
\input{../../WTHM/commands_WTHM}

\author{Willi Sontopski}

\parindent0cm %Ist wichtig, um führende Leerzeichen zu entfernen

\usepackage{color}

\usepackage{scrpage2}
\pagestyle{scrheadings}
\clearscrheadfoot

\ihead{Willi Sontopski}
\chead{WTHM WiSe 18 19}
\ohead{}
\ifoot{Aufgabenblatt 1}
\cfoot{Version: \today}
\ofoot{Seite \pagemark}

\begin{document}
%\setcounter{section}{1}

\section*{Aufgabe 1}
Sei
\begin{align*}
L_0^2:=\big\lbrace X\in L_2:\E[X]=0\big\rbrace
\end{align*}
der Raum der zentrierten quadrat-integrierbaren Zufallsvariablen. Für diesen gilt:
\begin{enumerate}[label=\alph*)]
\item $L_0^2$ ist ein Hilbertraum mit Skalarprodukt $\Cov(\cdot,\cdot)$.
\item Seien $X_1,...,X_n\in L_0^2$. Dann gilt:
\begin{align*}
\Var\left[\sum\limits_{j=1}^n X_j\right]
=\sum\limits_{j=1}^n\left(\Var[X_j]+2\cdot\sum\limits_{k>j}\Cov[X_j,X_k]\right)
\end{align*}
\item Seien $X_1,...,X_n\in L_0^2$. Dann gilt:
\begin{align*}
\sqrt{\Var\left[\sum\limits_{j=1}^n X_j\right]}\leq\sum\limits_{j=1}^n\sqrt{\Var[X_j]}
\end{align*}
\end{enumerate}
\begin{proof}
\underline{Zeige a):}\\
Nutze Untervektorraumkriterium:
\begin{itemize}
\item $0\in L_0^2$ ist klar.
\item Sei $X\in L_0^2$ und $\lambda\in\R$. Dann ist $X\in L_0^2$ wegen
\begin{align*}
\E[\lambda\cdot X]\stackeq{\text{Lin}}\lambda\cdot\E[X]\stackeq{X\in L_2^0}0
\end{align*}
\item Seien $X,Y\in L_0^2$. Dann ist $X+Y\in L_0^2$ wegen
\begin{align*}
\E[X+Y]\stackeq{\text{Lin}}\E[X]+\E[Y]\stackeq{X,Y\in L_0^2}0+0=0
\end{align*}
\end{itemize}
Somit ist $L_0^2$ ein Untervektorraum von $L_2$. Nun zeigen wir, dass
\begin{align*}
\langle X,Y\rangle:=\Cov(X,Y):=
\E\Big[\big(X-\E[X]\big)\cdot\big(Y-\E[Y]\big)\Big]
=\E[X\cdot Y]
\qquad\forall X,Y\in L_0^2
\end{align*}
ein Skalarprodukt auf $L_0^2$ ist. Gleich vorweg: Die Kovarianz erfüllt sogar auf $L^2$ alle Eigenschaften bis auf positive Definitheit. Diese gilt nur auf $L_0^2$. Deswegen werden im Folgenden die Eigenschaften der Kovarianz allgemeiner gezeigt.
\begin{itemize}
\item Die Kovarianz ist Bilinear, denn für $\lambda\in\R$ und $X,Y,Z\in L_0^2$ gilt
\begin{align*}
\Cov(\lambda\cdot X,Y)
&=\E\Big[\big(\lambda\cdot X-\E[ \lambda\cdot X]\big)\cdot\big(Y-\E[Y]\big)\Big]\\
&=\E\Big[\big(\lambda\cdot\big(X-\E[X]\big)\big)\cdot\big(Y-\E[Y]\big)\Big]\\
&=\lambda\cdot\Cov(X,Y)\\
\Cov(X,\lambda\cdot Y)
&=\E\Big[\big(X-\E[X]\big)\cdot\big(\lambda\cdot Y-\E[\lambda\cdot Y]\big)\Big]\\
&=\E\Big[\big(X-\E[X]\big)\cdot\big(\lambda\cdot\big(Y-\E[Y]\big)\big)\Big]\\
&=\lambda\cdot\Cov(X,Y)\\
\Cov(X+Z,Y)
&=\E\Big[\big(X+Z-\E[X+Z]\big)\cdot\big(Y-\E[Y]\big)\Big]\\
&=\E\Big[\big(X-\E[X]\big)\cdot\big(Y-\E[Y]\big)+\big(Z-\E[Z]\big)\cdot\big(Y-\E[Y]\big)\Big]\\
&=\E\Big[\big(X-\E[X]\big)\cdot\big(Y-\E[Y]\big)\Big]
+\E\Big[\big(Z-\E[Z]\big)\cdot\big(Y-\E[Y]\big)\Big]\\
&=\Cov(X,Y)+\Cov(Z,Y)\\
\Cov(X,Y+Z)
&=\E\Big[\big(X-\E[X]\big)\cdot\big(Y+Z-\E[Y+Z]\big)\Big]\\
&=\E\Big[\big(X-\E[X]\big)\cdot\big(Y-\E[Y]\big)+\big(X-\E[X]\big)\cdot\big(Z-\E[Z]\big)\Big]\\
&=\E\Big[\big(X-\E[X]\big)\cdot\big(Y-\E[Y]\big)\Big]
+\E\Big[\big(X-\E[X]\big)\cdot\big(Z-\E[Z]\big)\Big]\\
&=\Cov(X,Y)+\Cov(X,Z)
\end{align*}
\item Die Kovarianz ist symmetrisch, was man schon an der Definition erkennt: $\Cov(X,Y)=\Cov(Y,X)$
\item Die Kovarianz ist positiv definit:
\begin{align*}
\Cov(X,X)&=\E\Big[\underbrace{\big(X-\E[X]\big)^2}_{\geq0}\Big]\geq0\text{ und}\\
\Cov(X,X)&=0\stackrel{\E[X]=0}{\gdw} X=0\text{ fast sicher}
\end{align*}
\end{itemize}
Bleibt noch zu zeigen, dass $L_0^2$ bzgl. der die die Kovarianz induzierten Norm
\begin{align*}
\Vert X\Vert:=\sqrt{\Cov(X,X)}
=\sqrt{\E\Big[\big(X-\E[X]\big)^2\Big]}
=\sqrt{\E\Big[X^2\Big]}
\qquad\forall X,Y\in L_0^2
\end{align*}
vollständig ist. Diese Norm stimmt auf $L_0^2$ mit der bekannten $L^2$-Norm überein. Und da bekannt ist, dass $\big(L^2,\Vert\cdot\Vert_{L^2}\big)$ ein Banachraum ist, genügt es noch zu zeigen, dass $L_0^2$ abgeschlossen ist.
Sei also $(X_n)_{n\in\N}\subseteq L_0^2$ eine Folge mit
$X_n\stackrel{n\to\infty}{\longrightarrow}X\in L^2$.
Dann gilt schon $X\in L_0^2$, denn:
\begin{align*}
&\exists\left(X_{n_m}\right)_{m\in\N}\subseteq\left(X_n\right)_{n\in\N}:X_{n_m}\stackrel{\text{fast sicher}}{\longrightarrow}X\\
&\implies\E\left[\lim\limits_{m\to\infty}X_{n_m}-X\right]=0\\
&\implies
\E[X]=\E\left[X-X_{n_m}\right]+\underbrace{\E\left[X_{n_m}\right]}_{=0}\\
&\implies
\E[X]=\lim\limits_{m\to\infty}\E\left[X-X_{n_m}\right]
\stackeq{\text{dom Konv}}
\E\left[\lim\limits_{m\to\infty}\left(X-X_{n_m}\right)\right]
=0
\end{align*}

\underline{Zeige b):} Für $X,Y\in L_0^2$ gilt $\Var[X]=\E\big[X^2\big]$ und $\Cov[X,Y]=\E[X\cdot Y]$. Damit folgt
\begin{align*}
\Var\left[\sum\limits_{j=1}^n X_j\right]
&=\E\left[\left(\sum\limits_{j=1}^n X_j\right)^2\right]\\
&=\E\left[\sum\limits_{j=1}^n\sum\limits_{k=1}^n X_j\cdot X_k\right]\\
&=\sum\limits_{j=1}^n\sum\limits_{k=1}^n \E[X_j\cdot X_k]\\
&=\sum\limits_{j=1}^n\left(
\sum\limits_{\begin{subarray}{c}k=1\\ k=j\end{subarray}}^n \E[X_j\cdot X_k]
+\sum\limits_{\begin{subarray}{c}k=1\\ k\neq j\end{subarray}}^n \E[X_j\cdot X_k]\right)\\
&=\sum\limits_{j=1}^n\left(\underbrace{\E\big[X_j^2\big]}_{=\Var[X]}+2\cdot\sum\limits_{k>j}\underbrace{\E[X_j\cdot X_k]}_{=\Cov(X_j,Y_k)}\right)
\end{align*}

\underline{Zeige c):}\\
\begin{align*}
\sqrt{\Var\left[\sum\limits_{j=1}^n X_j\right]}
=\sqrt{\E\left[\left(\sum\limits_{j=1}^n\right)^2\right]}
=\left\Vert\sum\limits_{j=1}^n X_i\right\Vert
\stackrel{\Delta\text{-Ungl}}{\leq}
\sum\limits_{j=1}^n\Vert X_j\Vert
=\sum\limits_{j=1}^n\sqrt{\Var[X_j]}
\end{align*}
\end{proof}

\section*{Aufgabe 2}
Seien $X,Y\in L_2(\A)$ und $\F\subseteq\A$ Unter-$\sigma$-Algebra von $\A$. \textbf{Bedingte Varianz} und \textbf{bedingte Kovarianz} von $X$ bzw. $X,Y$ unter $\F$ sind definiert als
\begin{align*}
\Var[X~|~\F]&:=\E\Big[\big(X-\E[X~|~\F]\big)^2~\Big|~\F\Big]\\
\Cov[X,Y~|~\F]&:=\E\Big[\big(X-\E[X~|~\F]\big)\cdot\big(Y-\E[Y~|~\F]\big)~\Big|~\F\Big]
\end{align*}
Dann gelten die Sätze der \textbf{totalen Varianz} bzw. \textbf{totalen Kovarianz}:
\begin{align*}
\Var[X]&=\E\big[\Var[X~|~\F]\big]+\Var\big[\E[X~|~\F]\big]\\
\Cov[X,Y]&=\E\big[\Cov[X,Y~|~\F]\big]+\Cov\big[\E[X~|~F],\E[Y~|~\F]\big]
\end{align*}
\begin{proof}
\underline{Zur ersten Gleichung:}
\begin{align*}
\E\big[\Var[X~|~\F]\big]+\Var\big[\E[X~|~\F]\big]
&=\E\Big[\E\big[\big(X-\E[X~|~\F]\big)^2~\big|~\F\big]\Big]+\Var\big[\E[X~|~\F]\big]\\
&=ToDo
\end{align*}

\underline{Zur zweiten Gleichung:}
\begin{align*}
&\E\big[\Cov[X,Y~|~\F]\big]+\Cov\big[\E[X~|~F],\E[Y~|~\F]\big]\\
&=\E\Big[\E\big[\big(X-\E[X~|~\F]\big)\cdot\big(Y-\E[Y~|~\F]\big)~\big|~\F\big]\Big]+\Cov\big[\E[X~|~F],\E[Y~|~\F]\big]\\
&=ToDo
\end{align*}
\end{proof}

\section*{Aufgabe 3}
Sei $M=\begin{pmatrix}
A & B\\ C & D
\end{pmatrix}$ eine quadratische, in Blöcke unterteilte Matrix von vollem Rang, wobei $A$ und $D$ ebenfalls quadratisch und von vollem Rang seien. Die Ausdrücke
\begin{align*}
(M/A):=\left(D-C\cdot A^{-1}\cdot B\right),\qquad(M/D):=\left(A-B\cdot D^{-1}\cdot C\right)
\end{align*}
heißen \textbf{Schurkomplement} von $A$ in $M$ bzw. $D$ in $M$. Dann giltt:
\begin{enumerate}[label=\alph*)]
\item $\begin{aligned}
M^{-1}=\begin{pmatrix}
(M/D)^{-1} & -A^{-1}\cdot B\cdot(M/A)^{-1}\\
-D^{-1}\cdot C\cdot(M/D)^{-1} & (M/A)^{-1}
\end{pmatrix}
\end{aligned}$
\item Sei $M$ nun symmetrisch, d.h. $A$ und $D$ sind symmetrisch und $C=B^T$. Dann gilt:
\begin{align*}
(x^T,y^T)\cdot M^{-1}\cdot\begin{pmatrix}
x\\y
\end{pmatrix}-y^T\cdot D^{-1}\cdot y=\tilde{x}^T\cdot(M/D)^{-1}\cdot\tilde{x}
\end{align*}
mit $\tilde{x}=\left(x-B\cdot D^{-1}\cdot y\right)$ für alle $x,y$ mit passender Dimension.
\item Es sei $(X,Y)$ multivariat normalverteilt mit Erwartungswert 0 und positiv definiter Kovarianzmatrix $\Sigma=\begin{pmatrix}
\Sigma_X & \Sigma_{XY}\\ \Sigma_{XY}^T & \Sigma_Y
\end{pmatrix}$. Dann ist $X$ bedingt auf $Y$ normalverteilt mit $\E[X~|~Y]=\Sigma_{XY}\cdot\Sigma_{Y}^{-1}$ und Kovarianzmatrix $(\Sigma/\Sigma_Y)$\\
Hinweis: es gilt $\det(\Sigma)=\det(\Sigma_Y)\cdot\det(\Sigma/\Sigma_Y)$.
\end{enumerate}
\begin{proof}
\underline{Zeige a):}\\ ToDo

\underline{Zeige b):}\\

\underline{Zeige c):}\\
\end{proof}

\section*{Aufgabe 4}
In einer bestimmten Population sei das Alter $X$ bei erstmaliger
Berufsunfähigkeit exponentialverteilt mit Parameter $\lambda>0$. Für eine Versicherungsgesellschaft die gegen Berufsunfähigkeit versichert ist das mittlere Alter bei Eintritt der Berufsunfähigkeit von Bedeutung, unter der Bedingung dass die Berufsunfähigkeit zwischen den Altersgrenzen $0\leq a\leq b$ eintritt.\\
Bestimme diesen bedingten Erwartungswert $\E[X~|~a\leq X\leq b]$
\begin{proof}
\begin{align*}
\E[X~|~a\leq X\leq b]
&=\frac{\E\left[X\cdot\indi_{\lbrace a\leq X\leq b\rbrace}\right]}{\P[a\leq X\leq b]}\\
&=\frac{\int\limits_a^b \lambda\cdot x\cdot\exp(-\lambda\cdot x)\d x}{\int\limits_a^b\lambda\cdot\exp(-\lambda\cdot x)\d x}\\
&=\frac{\int\limits_a^b x\cdot\exp(-\lambda\cdot x)\d x}{\int\limits_a^b\exp(-\lambda\cdot x)\d x}\\
&=\frac{\left[\left(-\frac{x}{\lambda}-\frac{1}{\lambda^2}\right)\cdot\exp(-\lambda\cdot x)\right]_{x=a}^b}
{\left[-\frac{\exp(-\lambda\cdot x)}{\lambda}\right]_{x=a}^b}\\
%&=\frac{
%\frac{b\cdot\exp(-\lambda\cdot b)}{\lambda}
%+\frac{\exp(-\lambda\cdot b)}{\lambda^2}
%-\frac{a\cdot\exp(-\lambda\cdot a)}{\lambda}
%-\frac{\exp(-\lambda)\cdot a)}{\lambda^2}
%}{
%\frac{\exp(-\lambda\cdot b)}{\lambda}
%-\frac{\exp(-\lambda\cdot a)}{\lambda}}\\
%&=\frac{\left(b+\frac{1}{\lambda}\right)\cdot\exp(-\lambda\cdot b)-\left(a+\frac{1}{\lambda}\right)\cdot\exp(-\lambda\cdot a)}{\exp(-\lambda\cdot b)-\exp(-\lambda\cdot a)}
&=\frac{\frac{(a\cdot\lambda+1)\cdot\exp(-\lambda\cdot a)}{\lambda^2}-\frac{(b\cdot\lambda+1)\cdot\exp(-\lambda\cdot b)}{\lambda^2}}{\frac{\exp(-\lambda\cdot a)}{\lambda}-\frac{\exp(-\lambda\cdot b)}{\lambda}}\\
&=\frac{(b\cdot\lambda+1)\cdot\exp(a\cdot\lambda)-(a\cdot\lambda+1)\cdot\exp(b\cdot\lambda)}{\big(\exp(a\cdot\lambda)-\exp(b\cdot\lambda)\big)\cdot\lambda}
\end{align*}
\end{proof}

\section*{Aufgabe 5}
Welche der folgenden in der Vorlesung definierten mathematischen
Objekte sind: reelle Zahlen, Zufallsvariablen, meßbare Funktionen von
$\R\to\R$?
\begin{align*}
\E[X~|~\F],\qquad\P[A~|~B],\qquad \E[X~|~Y=y],\qquad\P[A~|~\F],\qquad\E[X~|~Y]
\end{align*}
Wie üblich bezeichnet $\F$ eine $\sigma$-Algebra, $X,Y$  Zufallsvariablen, $y$ eine reelle Zahl und $A,B$ Ereignisse.
\begin{lösung}
\begin{align*}
&\E[X~|~\F]\in L_2(\Omega,\F,\P)\text{, ist also eine Zufallsvariable}\\
&\P[A~|~B]\in[0,1]\subseteq\R\\
&\E[X~|~Y=y]:=\int\limits_{\R^m}x\cdot f_{X|Y}(x,y)\d x\in\R\\
&\P[A~|~\F]:=\E[\indi_A~|~\F]\in L_2(\Omega,\F,P)\text{ bzw. }\in(0,1)\text{ für festes }\omega\in\Omega\\
&\E[X~|~Y]:=\E[X~|~\sigma(Y)]\in L_2(\Omega,\F,\P)\text{, ist also ein Spezialfall der ersten Zeile}
\end{align*}
\end{lösung}
\end{document}