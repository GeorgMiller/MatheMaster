% This work is licensed under the Creative Commons
% Attribution-NonCommercial-ShareAlike 4.0 International License. To view a copy
% of this license, visit http://creativecommons.org/licenses/by-nc-sa/4.0/ or
% send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.

\documentclass[12pt,a4paper]{article} 

\input{../../latex/packages}
\input{../../latex/theoremenvironments}
\input{../../latex/commands}
\input{../../latex/commands_Willi}
\input{../../latex/commands_stochastik}
\input{../../WTHM/commands_WTHM}

\author{Willi Sontopski}

\parindent0cm %Ist wichtig, um führende Leerzeichen zu entfernen

\usepackage{scrpage2}
\pagestyle{scrheadings}
\clearscrheadfoot

\ihead{Willi Sontopski \& Robert Walter}
\chead{}
\ohead{WTHM WiSe 18 19}
\ifoot{Aufgabenblatt 4}
\cfoot{Version: \today}
\ofoot{Seite \pagemark}

\begin{document}
\section*{Aufgabe 1}
Sei $(X_n)_{n\in\N}$ eine Folge von iid Zufallsvariablen in $L_1$ und 
\begin{align*}
R_n:=\frac{1}{n}\cdot\sum\limits_{j=0}^n X_j,\qquad
\G_n:=\sigma\big(R_n,R_{n+1},R_{n+2},\ldots\big)\qquad\forall n\in\N
\end{align*}
Dann gilt:
\begin{enumerate}[label=\alph*)]
\item $\G_n=\sigma\big(R_n,X_{n+1},X_{n+2},\ldots\big)$
\item $(R_n)_{n\in\N}$ ist Rückwärtsmartingal bzgl. $(\G_n)_{n\in\N}$
\item Es gilt das starke Gesetz der großen Zahlen:
\begin{align*}
\limn R_n=\E[X_1]\text{ f.s.}
\end{align*}
\end{enumerate}

\begin{proof}
\underline{Zeige a):}\\
Wegen der Definition
\begin{align*}
\sigma\big(R_n,R_{n+1},R_{n+2},\ldots\big)
\stackeq{\text{Def}}\sigma\Big(\big\lbrace R_i^{-1}(A):A\in\B(\R),i\geq n\big\rbrace\Big)
\end{align*}
genügt es zu zeigen, dass 
\begin{align*}
\left\lbrace X_i^{-1}(A):A\in\B(\R),i>n\right\rbrace=\left\lbrace R_i^{-1}(A):A\in\B(\R),i>n\right\rbrace
\end{align*}
gilt. Sei also...\nl
%TODO
\underline{Zeige b):}\\
\begin{enumerate}
\item $(\G_n)_{n\in\N}$ ist eine absteigende Folge, denn nach Definition gilt $\G_{n+1}\subseteq\G_n~\forall n\in\N$.
\item Für beliebiges $n\in\N$ ist $R_n$ $\G_n$-messbar (nach Konstruktion von $(\G_n)_{n\in\N}$).
\item $\E\big[|R_n|]<\infty~\forall n\in\N$, denn:
\begin{align*}
\E\big[|R_n|\big]
=\E\left[\left|\frac{1}{n}\cdot\sum\limits_{j=0}^n X_j\right|\right]\stackrelnew{\text{Mono}}{\text{DU}}{\leq}
\E\left[\frac{1}{n}\cdot\sum\limits_{j=0}^n\big|X_j\big|\right]\overset{\text{Lin}}{=}
\frac{1}{n}\cdot\sum\limits_{j=0}^n\underbrace{\E\big[|X_j|\big]}_{\underset{X_j\in L_1}{<}\infty}<\infty
\end{align*} 
\item Es gilt $R_n=\E\big[R_k~\big|~\G_n\big]~\forall k\leq n$, denn: %TODO
\begin{align*}
\E\big[R_k~\big|~\G_n\big]
\overset{\text{Def}}&=
\E\left[\left.\frac{1}{k}\cdot\sum\limits_{j=0}^k X_j~\right|~\G_n\right]\\
\overset{\text{Lin}}&=
\frac{1}{k}\cdot\sum\limits_{j=0}^k\E\big[X_j~\big|~\G_n\big]\\
&=\ldots\\
&=\frac{1}{n}\cdot\sum\limits_{j=0}^n X_j\\
\overset{\text{Def}}&=
R_n
\end{align*}
\end{enumerate}
\underline{Zeige c):}\\ %TODO Idee ist da, Details fehlen
Da $(R_n)_{n\in\N}$ nach b) Rückwärtsmartingal ist, konvergiert der stochastische Prozess $(R_n)_{n\in\N}$ nach Theorem 5.5. Sei die Zufallsvariable $R_\infty\in L_1(\G_\infty)$ dieser Grenzwert.\\
Da die Zufallsvariablen $X_1,X_2,\ldots$ unabhängig sind  (SIND SIE DAS???), können wir das 0-1-Gesetz von Kolmogorov anwenden und erhalten, dass $\G_\infty$ $\P$-trivial ist. Folgich (WARUM???) ist die Grenzufallsvarible $R_\infty$ fas t sicher konstant, also $R_\infty\equiv c$ f.s. für ein $c\in\R$.\\
Aus der Rückwärtsmartingaleigenschaft folgt:
\begin{align*}
R_n&=\E\big[R_1~\big|~\G_n\big]=\E\big[X_1~\big|~\G_n\big] &\forall n\in\N\\
\overset{\E[\cdot]}{\implies}
\E\big[R_n\big]&=\E\Big[\E\big[X_1~\big|~\G_n\big]\Big]\overset{\text{Tower}}{=}\E\big[X_1\big] &\forall n\in\N\\
\implies c&=\E[c]=\E[\R_\infty]=\E\left[\limn R_n\right]\\\overset{???}&{=}\limn\E[R_n]=\limn\E[X_1]=\E[X_1]\\
\implies\limn R_n \overset{\text{f.s.}}&=\limn\E[R_n]=\E[X_1]\text{ f.s.}
\end{align*}
\end{proof}

\section*{Aufgabe 2}
Wir berechnen die charakteristische Funktionen folgender Verteilungen:
\begin{enumerate}[label=a)]
\item Binomial-, Poisson- und geometrische Verteilung
\item Exponential-, Laplace- und Cauchyverteilung
\end{enumerate}

\begin{lösung}
\underline{Zur Binomialverteilung:} Sei $X\sim\text{Bin}(p,n)$. Dann gilt:
%TODO
\begin{align*}
\Phi_{\text{Bin}}(\xi)
\overset{\text{Def}}&{=}
\E\big[\exp(i\cdot X\cdot\xi)\big]\\
&=\displaystyle\sum^n_{k=0} e^{ik\xi} \binom{n}{k} p^{k}q^{n-k}\\
&=\displaystyle\sum^n_{k=0} \binom{n}{k} \left(pe^{i\xi}\right)^k  q^{n-k}\\
&=\big(p\cdot\exp(i\cdot \xi)+1\big)^n
\end{align*}
\underline{Zur Poissonverteilung:} Sei $X\sim\text{Poi}(\lambda)$. Dann gilt:
%TODO
\begin{align*}
\Phi_{\text{Poi}}(\xi)
\overset{\text{Def}}&{=}
\E\big[\exp(i\cdot X\cdot\xi)\big]\\
&=\displaystyle\sum^n_{k=0} e^{ik\xi} e^{-\lambda}\frac{\lambda^k}{k!}\\
&=e^{-\lambda} \displaystyle\sum^n_{k=0}  \frac{\left( \lambda e^{i\xi} \right)^k}{k!}\\
&=e^{-\lambda} e^{\lambda e^{i\xi} }\\
&=\exp\big(\lambda \left( \exp(i\cdot \xi) -1 \right)\big)
\end{align*}
\underline{Zur geometrischen Verteilung:} Sei $X\sim\text{Geo}(p)$. Dann gilt mit $q = 1-p$:
%TODO
\begin{align*}
\Phi_{\text{Geo}}(\xi)
\overset{\text{Def}}&{=}
\E\big[\exp(i\cdot X\cdot\xi)\big]\\
&=\displaystyle\sum^n_{k=1} e^{ik\xi} pq^{k-1}\\
&=\displaystyle\sum^n_{k=0} e^{i\xi(k+1)} pq^k\\
&=\displaystyle\sum^n_{k=0} (e^{i\xi q })^k e^{i\xi}p\\
&=\frac{p\cdot\exp(i\cdot \xi)}{1-q\cdot\exp(i\cdot \xi)}
\end{align*}
\underline{Zur Exponentialverteilung:} Sei $X\sim\text{Exp}(\lambda)$. Dann gilt:
%TODO
\begin{align*}
\Phi_{\text{Exp}}(\xi)
\overset{\text{Def}}&{=}
\E\big[\exp(i\cdot X\cdot\xi)\big]\\
&=\int \limits_{\R_{\geq 0}} e^{i\xi x}\lambda e^{-\lambda x} \d x\\
&= \lambda \left[ \frac{1}{i\xi-\lambda} e^{i\xi-\lambda}\right]^\infty_0 \\
&= \frac{\lambda}{i\xi -\lambda}\left(0-1\right)\\
&=\frac{\lambda}{\lambda-i\cdot \xi}
\end{align*}
\underline{Zur Laplaceverteilung:} Sei $X\sim\text{Lap}$. Dann gilt:
%TODO
\begin{align*}
\Phi_{\text{Lap}}(\xi)
\overset{\text{Def}}&{=}
\E\big[\exp(i\cdot X\cdot\xi)\big]\\
&=\int \limits_{\R} e^{i\xi x}\frac{1}{2} e^{-|x|} \d x\\
&=\frac{1}{2} \left( \int \limits_{\R_{\geq 0}} e^{(i\xi-1)x}\d x + \int \limits_{\R_{\leq 0}} e^{(i\xi+1)x}\d x \right)\\
&=\frac{1}{2} \left( \left[ \frac{1}{i\xi-1} e^{(i\xi-1)x} \right]^\infty_0 + \left[ \frac{1}{i\xi+1} e^{(i\xi+1)x} \right]^0_{-\infty} \right)\\
&=\frac{1}{2} \left( -\frac{1}{i\xi-1} + \frac{1}{i\xi+1} \right)\\
&=\frac{1}{1+\xi^2}
\end{align*}
\underline{Zur Cauchyverteilung:} Sei $X\sim\text{Cauchy}$. Dann gilt:
%TODO
\begin{align*}
\Phi_{\text{Cauchy}}(\xi)
\overset{\text{Def}}&{=}
\E\big[\exp(i\cdot X\cdot\xi)\big]\\
&=\ldots\\
&=\exp\big(-|\xi|\big)
\end{align*}
\end{lösung}

\section*{Aufgabe 3}
Seien $X$ und $X'$ unabhängige $\R^d$-wertige Zufallsvariablen mit der gleichen Verteilung $F$.
\begin{enumerate}[label=\alph*)]
\item Berechne die charakteristische Funktion von $Y=X-X'$.
\item Sei $B$ unabhängig von $(X,X')$ mit $\P(B=0)=\P(B=1)=\frac{1}{2}$. Berechne die charakteristische Funktion von $Z=B\cdot X+(B-1)\cdot X'$.
\end{enumerate}
Sind die Zufallsvariablen $Y$ und $Z$ symmetrisch?

\begin{lösung}
\underline{Zu a):}
%TODO
\begin{align*}
\Phi_{Y}(\xi)
\overset{\text{Def}}&{=}
\E\big[\exp(i\cdot (X-X')\cdot\xi)\big]\\
&=\ldots\\
\end{align*}
\underline{Zu b):}
%TODO
\begin{align*}
\Phi_{Z}(\xi)
\overset{\text{Def}}&{=}
\E\big[\exp(i\cdot (B\cdot X+(B-1)\cdot X')\cdot\xi)\big]\\
&=\ldots\\
\end{align*}
\underline{Zur Symmetrie:}\\
%TODO
\end{lösung}

\section*{Aufgabe 4}
Sei $\alpha\in\N_0^d$ ein Multiindex, $b\in\R^d$ und $f,g$ Funktionen in $C^\alpha(\R^d)$.
\begin{enumerate}[label=\alph*)]
\item Berechne $\partial_x^\alpha\exp\big(b^T\cdot x\big)$
\item Zeige die Leibniz-Regel:
\begin{align*}
\partial^\alpha(f\cdot g)=\sum\limits_{\begin{subarray}{c}\beta\leq\alpha\\\beta\in\N_0^d\end{subarray}}\begin{pmatrix}
\alpha\\\beta
\end{pmatrix}\cdot\partial^\beta f\cdot\partial^{\alpha-\beta}g
\end{align*}
\end{enumerate}

\begin{lösung}
\underline{Zu a):}
Wegen
\begin{align*}
\frac{\partial}{\partial_{x_i}}\exp(b^T\cdot x)
&=\frac{\partial}{\partial_{x_i}}\exp\left(\sum\limits_{i=1}^d b_i\cdot x_i\right)
=b_i\cdot\exp\big(b_i\cdot x_i\big)\\
\implies\frac{\partial^k}{\partial_{x_i}^k}\exp(b^T\cdot x)
&=b_i^k\cdot\exp\big(b_i\cdot x_i\big)
\end{align*}
gilt
\begin{align*}
\partial_x^\alpha\exp\big(b^T\cdot x\big)
\overset{\text{Def}}&{=}
\frac{\partial^{\alpha_1}\ldots\partial^{\alpha_d}}{\partial_{x_1}^{\alpha_1}\ldots\partial_{x_d}^{\alpha_d}}\exp(b^T\cdot x)
=b_i^{\alpha_1}\cdot\ldots\cdot b_d^{\alpha_d}\cdot\exp\big(b^T\cdot x\big)\\
&=b^\alpha \exp\big(b^T\cdot x\big)
\end{align*}
%TODO Das ist möglicherweise nicht ganz richtig. Bin mir sehr unsicher
\underline{Zu b):}\\
Wir führen eine Induktion nach $|\alpha|\in\N$ durch:\nl
\ul{IA:} Sei $|\alpha|=0$ %TODO
\nl
\ul{IV:} Gelte die Leibniz-Regel für beliebiges, aber festes $\alpha\in\N_0^d$ mit $|\alpha|=n\in\N$.\nl
\ul{IS:}
%TODO
\end{lösung}

\section*{Aufgabe 5}
\begin{enumerate}[label=\alph*)]
\item Sei $(X,Y)$ bivariat normalverteilt. Zeige mit charakteristischen Funktionen: $X$ und $Y$ sind unabhängig genau dann, wenn sie unkorreliert sind.
\item Sei $X$ Cauchyverteilt und setze $Y=X$. Zeige, dass für die charakteristischen Funktionen gilt:
\begin{align*}
\Phi_{X+Y}(\xi)=\Phi_X(\xi)\cdot\Phi_Y(\xi)\qquad\forall\xi\in\R
\end{align*} 
obwohl $X$ offensichtlich \ul{nicht} unabhängig von $Y$ ist.
\end{enumerate}

\begin{lösung}
\underline{Zu a):}\\
%TODO
\underline{Zu b):}\\
%TODO
\end{lösung}
\end{document}
